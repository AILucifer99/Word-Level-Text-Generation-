{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Word-Level-Text-Generation-LSTM-Embedding.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"17QEf8G_PXLCkKJTTbgetx_bwgylIhqcv","authorship_tag":"ABX9TyM4CbYnrls+/uXUhYjCm7xR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"7g6NBTWug3fT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650439466159,"user_tz":-330,"elapsed":43961,"user":{"displayName":"VR AI IEMA","userId":"08782298689493191086"}},"outputId":"de536112-3347-4bdd-e166-ba7d3d8431e6"},"outputs":[{"output_type":"stream","name":"stdout","text":["The Project Gutenberg eBook of The Republic, by Plato\n","\n","This eBook is for the use of anyone anywhere in the United States and\n","most other parts of the world at no cost and with almost no restrictions\n","whatsoever. You may copy it, give it away or re-use it under the terms\n","of the Project Gutenberg License included with this eBook or online at\n","www.gutenberg.org. If you are not located in the United States, you\n","will have to check the laws of the country where you are located before\n","using this eBook.\n","\n","Title: The Republic\n","\n","Author: Plato\n","\n","Translator: B. Jowett\n","\n","Release Date: October, 1998 [eBook #1497]\n","[Most recently updated: September 11, 2021]\n","\n","Language: English\n","\n","\n","Produced by: Sue Asscher and David Widger\n","\n","*** START OF THE PROJECT GUTENBERG EBOOK THE REPUBLIC ***\n","\n","\n","\n","\n","THE REPUBLIC\n","\n","By Plato\n","\n","Translated by Benjamin Jowett\n","\n","Note: See also “The Republic” by Plato, Jowett, eBook #150\n","\n","\n","Contents\n","\n"," INTRODUCTION AND ANALYSIS.\n"," THE REPUBLIC.\n"," PERSONS OF THE DIALOGUE.\n"," BOOK I.\n"," BOOK II.\n"," BOOK III.\n"," BOOK IV.\n"," BOOK V.\n"," BOOK VI.\n"," BOOK VII.\n"," BOOK VIII.\n"," BOOK IX.\n"," BOOK X.\n","\n","\n","\n","\n"," INTRODUCTION AND ANALYSIS.\n","\n","\n","The Republic of Plato is the longest of his works with the exception of\n","the Laws, and is certainly the greatest of them. There are nearer\n","approaches to modern metaphysics in the Philebus and in the Sophist;\n","the Politicus or Statesman is more ideal; the form and institutions of\n","the State are more clearly drawn out in the Laws; as works of art, the\n","Symposium and the Protagoras are of higher excellence. But no other\n","Dialogue of Plato has the same largeness of view and the same\n","perfection of style; no other shows an equal knowledge of the world, or\n","contains more of those thoughts which are new as well as old, and not\n","of one age only but of all. Nowhere in Plato is there a deeper irony or\n","a greater wealth of humour or imagery, or more dramatic power. Nor in\n","any other of his writings is the attempt made to interweave life and\n","speculation, or to connect politics with philosophy. The Republic is\n","<class 'str'>\n","['the', 'project', 'gutenberg', 'ebook', 'of', 'the', 'republic', 'by', 'plato', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'united', 'states', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 'reuse', 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'wwwgutenbergorg', 'if', 'you', 'are', 'not', 'located', 'in', 'the', 'united', 'states', 'you', 'will', 'have', 'to', 'check', 'the', 'laws', 'of', 'the', 'country', 'where', 'you', 'are', 'located', 'before', 'using', 'this', 'ebook', 'title', 'the', 'republic', 'author', 'plato', 'translator', 'b', 'jowett', 'release', 'date', 'october', 'ebook', 'most', 'recently', 'updated', 'september', 'language', 'english', 'produced', 'by', 'sue', 'asscher', 'and', 'david', 'widger', 'start', 'of', 'the', 'project', 'gutenberg', 'ebook', 'the', 'republic', 'the', 'republic', 'by', 'plato', 'translated', 'by', 'benjamin', 'jowett', 'note', 'see', 'also', 'by', 'plato', 'jowett', 'ebook', 'contents', 'introduction', 'and', 'analysis', 'the', 'republic', 'persons', 'of', 'the', 'dialogue', 'book', 'i', 'book', 'ii', 'book', 'iii', 'book', 'iv', 'book', 'v', 'book', 'vi', 'book', 'vii', 'book', 'viii', 'book', 'ix', 'book', 'x', 'introduction', 'and', 'analysis', 'the', 'republic', 'of', 'plato', 'is', 'the', 'longest', 'of', 'his', 'works', 'with', 'the', 'exception', 'of', 'the', 'laws', 'and', 'is', 'certainly', 'the', 'greatest', 'of', 'them', 'there', 'are', 'nearer', 'approaches', 'to', 'modern', 'metaphysics', 'in', 'the', 'philebus', 'and', 'in', 'the', 'sophist', 'the', 'politicus', 'or', 'statesman', 'is', 'more', 'ideal', 'the', 'form', 'and', 'institutions', 'of', 'the', 'state', 'are', 'more', 'clearly', 'drawn', 'out', 'in', 'the', 'laws', 'as', 'works', 'of', 'art', 'the', 'symposium', 'and', 'the', 'protagoras', 'are', 'of', 'higher', 'excellence', 'but', 'no', 'other', 'dialogue', 'of', 'plato', 'has', 'the', 'same', 'largeness', 'of', 'view', 'and', 'the', 'same', 'perfection', 'of', 'style', 'no', 'other', 'shows', 'an', 'equal', 'knowledge', 'of', 'the', 'world', 'or', 'contains', 'more', 'of', 'those', 'thoughts', 'which', 'are', 'new', 'as', 'well', 'as', 'old', 'and', 'not', 'of', 'one', 'age', 'only', 'but', 'of', 'all', 'nowhere', 'in', 'plato', 'is', 'there', 'a', 'deeper', 'irony', 'or', 'a', 'greater', 'wealth', 'of', 'humour', 'or', 'imagery', 'or', 'more', 'dramatic', 'power', 'nor', 'in', 'any', 'other', 'of', 'his', 'writings', 'is', 'the', 'attempt', 'made', 'to', 'interweave', 'life', 'and', 'speculation', 'or', 'to', 'connect', 'politics', 'with', 'philosophy', 'the', 'republic', 'is', 'the', 'centre', 'around', 'which', 'the', 'other', 'dialogues', 'may', 'be', 'grouped', 'here', 'philosophy', 'reaches', 'the', 'highest', 'point', 'cp', 'especially', 'in', 'books', 'v', 'vi', 'vii', 'to', 'which', 'ancient', 'thinkers', 'ever', 'attained', 'plato', 'among', 'the', 'greeks', 'like', 'bacon', 'among', 'the', 'moderns', 'was', 'the', 'first', 'who', 'conceived', 'a', 'method', 'of', 'knowledge', 'although', 'neither', 'of', 'them', 'always', 'distinguished', 'the', 'bare', 'outline', 'or', 'form', 'from', 'the', 'substance', 'of', 'truth', 'and', 'both', 'of', 'them', 'had', 'to', 'be', 'content', 'with', 'an', 'abstraction', 'of', 'science', 'which', 'was', 'not', 'yet', 'realized', 'he', 'was', 'the', 'greatest', 'metaphysical', 'genius', 'whom', 'the', 'world', 'has', 'seen', 'and', 'in', 'him', 'more', 'than', 'in', 'any', 'other', 'ancient', 'thinker', 'the', 'germs', 'of', 'future', 'knowledge', 'are', 'contained', 'the', 'sciences', 'of', 'logic', 'and', 'psychology', 'which', 'have', 'supplied', 'so', 'many', 'instruments', 'of', 'thought', 'to', 'afterages', 'are', 'based', 'upon', 'the', 'analyses', 'of', 'socrates', 'and', 'plato', 'the', 'principles', 'of', 'definition', 'the', 'law', 'of', 'contradiction', 'the', 'fallacy', 'of', 'arguing', 'in', 'a', 'circle', 'the', 'distinction', 'between', 'the', 'essence', 'and', 'accidents', 'of', 'a', 'thing', 'or', 'notion', 'between', 'means', 'and', 'ends', 'between', 'causes', 'and', 'conditions', 'also', 'the', 'division', 'of', 'the', 'mind', 'into', 'the', 'rational', 'concupiscent', 'and', 'irascible', 'elements', 'or', 'of', 'pleasures', 'and', 'desires', 'into', 'necessary', 'and', 'and', 'other', 'great', 'forms', 'of', 'thought', 'are', 'all', 'of', 'them', 'to', 'be', 'found', 'in', 'the', 'republic', 'and', 'were', 'probably', 'first', 'invented', 'by', 'plato', 'the', 'greatest', 'of', 'all', 'logical', 'truths', 'and', 'the', 'one', 'of', 'which', 'writers', 'on', 'philosophy', 'are', 'most', 'apt', 'to', 'lose', 'sight', 'the', 'difference', 'between', 'words', 'and', 'things', 'has', 'been', 'most', 'strenuously', 'insisted', 'on', 'by', 'him', 'cp', 'rep', 'polit', 'cratyl', 'ff', 'although', 'he', 'has', 'not', 'always', 'avoided', 'the', 'confusion', 'of', 'them', 'in', 'his', 'own', 'writings', 'eg', 'rep', 'but', 'he', 'does', 'not', 'bind', 'up', 'truth', 'in', 'logical', 'is', 'still', 'veiled', 'in', 'metaphysics', 'and', 'the', 'science', 'which', 'he', 'imagines', 'to', 'all', 'truth', 'and', 'all', 'is', 'very', 'unlike', 'the', 'doctrine', 'of', 'the', 'syllogism', 'which', 'aristotle', 'claims', 'to', 'have', 'discovered', 'soph', 'elenchi', 'neither', 'must', 'we', 'forget', 'that', 'the', 'republic', 'is', 'but', 'the', 'third', 'part', 'of', 'a', 'still', 'larger', 'design', 'which', 'was', 'to', 'have', 'included', 'an', 'ideal', 'history', 'of', 'athens', 'as', 'well', 'as', 'a', 'political', 'and', 'physical', 'philosophy', 'the', 'fragment', 'of', 'the', 'critias', 'has', 'given', 'birth', 'to', 'a', 'worldfamous', 'fiction', 'second', 'only', 'in', 'importance', 'to', 'the', 'tale', 'of', 'troy', 'and', 'the', 'legend', 'of', 'arthur', 'and', 'is', 'said', 'as', 'a', 'fact', 'to', 'have', 'inspired', 'some', 'of', 'the', 'early', 'navigators', 'of', 'the', 'sixteenth', 'century', 'this', 'mythical', 'tale', 'of', 'which', 'the', 'subject', 'was', 'a', 'history', 'of', 'the', 'wars', 'of', 'the', 'athenians', 'against', 'the', 'island', 'of', 'atlantis', 'is', 'supposed', 'to', 'be', 'founded', 'upon', 'an', 'unfinished', 'poem', 'of', 'solon', 'to', 'which', 'it', 'would', 'have', 'stood', 'in', 'the', 'same', 'relation', 'as', 'the', 'writings', 'of', 'the', 'logographers', 'to', 'the', 'poems', 'of', 'homer', 'it', 'would', 'have', 'told', 'of', 'a', 'struggle', 'for', 'liberty', 'cp', 'tim', 'c', 'intended', 'to', 'represent', 'the', 'conflict', 'of', 'persia', 'and', 'hellas', 'we', 'may', 'judge', 'from', 'the', 'noble', 'commencement', 'of', 'the', 'timaeus', 'from', 'the', 'fragment', 'of', 'the', 'critias', 'itself', 'and', 'from', 'the', 'third', 'book', 'of', 'the', 'laws', 'in', 'what', 'manner', 'plato', 'would', 'have', 'treated', 'this', 'high', 'argument', 'we', 'can', 'only', 'guess', 'why', 'the', 'great', 'design', 'was', 'abandoned', 'perhaps', 'because', 'plato', 'became', 'sensible', 'of', 'some', 'incongruity', 'in', 'a', 'fictitious', 'history', 'or', 'because', 'he', 'had', 'lost', 'his', 'interest', 'in', 'it', 'or', 'because', 'advancing', 'years', 'forbade', 'the', 'completion', 'of', 'it', 'and', 'we', 'may', 'please', 'ourselves', 'with', 'the', 'fancy', 'that', 'had', 'this', 'imaginary', 'narrative', 'ever', 'been', 'finished', 'we', 'should', 'have', 'found', 'plato', 'himself', 'sympathising', 'with', 'the', 'struggle', 'for', 'hellenic', 'independence', 'cp', 'laws', 'iii', 'ff', 'singing', 'a', 'hymn', 'of', 'triumph', 'over', 'marathon', 'and', 'salamis', 'perhaps', 'making', 'the', 'reflection', 'of', 'herodotus', 'v', 'where', 'he', 'contemplates', 'the', 'growth', 'of', 'the', 'athenian', 'brave', 'a', 'thing', 'is', 'freedom', 'of', 'speech', 'which', 'has', 'made', 'the', 'athenians', 'so', 'far', 'exceed', 'every', 'other', 'state', 'of', 'hellas', 'in', 'or', 'more', 'probably', 'attributing', 'the', 'victory', 'to', 'the', 'ancient', 'good', 'order', 'of', 'athens', 'and', 'to', 'the', 'favor', 'of', 'apollo', 'and', 'athene', 'cp', 'introd', 'to', 'critias', 'again', 'plato', 'may', 'be', 'regarded', 'as', 'the', 'or', 'leader', 'of', 'a', 'goodly', 'band', 'of', 'followers', 'for', 'in', 'the', 'republic', 'is', 'to', 'be', 'found', 'the', 'original', 'of', 'de', 'republica', 'of', 'st', 'city', 'of', 'god', 'of', 'the', 'utopia', 'of', 'sir', 'thomas', 'more', 'and', 'of', 'the', 'numerous', 'other', 'imaginary', 'states', 'which', 'are', 'framed', 'upon', 'the', 'same', 'model', 'the', 'extent', 'to', 'which', 'aristotle', 'or', 'the', 'aristotelian', 'school', 'were', 'indebted', 'to', 'him', 'in', 'the', 'politics', 'has', 'been', 'little', 'recognised', 'and', 'the', 'recognition', 'is', 'the', 'more', 'necessary', 'because', 'it', 'is', 'not', 'made', 'by', 'aristotle', 'himself', 'the', 'two', 'philosophers', 'had', 'more', 'in', 'common', 'than', 'they', 'were', 'conscious', 'of', 'and', 'probably', 'some', 'elements', 'of', 'plato', 'remain', 'still', 'undetected', 'in', 'aristotle', 'in', 'english', 'philosophy', 'too', 'many', 'affinities', 'may', 'be', 'traced', 'not', 'only', 'in', 'the', 'works', 'of', 'the', 'cambridge', 'platonists', 'but', 'in', 'great', 'original', 'writers', 'like', 'berkeley', 'or', 'coleridge', 'to', 'plato', 'and', 'his', 'ideas', 'that', 'there', 'is', 'a', 'truth', 'higher', 'than', 'experience', 'of', 'which', 'the', 'mind', 'bears', 'witness', 'to', 'herself', 'is', 'a', 'conviction', 'which', 'in', 'our', 'own', 'generation', 'has', 'been', 'enthusiastically', 'asserted', 'and', 'is', 'perhaps', 'gaining', 'ground', 'of', 'the', 'greek', 'authors', 'who', 'at', 'the', 'renaissance', 'brought', 'a', 'new', 'life', 'into', 'the', 'world', 'plato', 'has', 'had', 'the', 'greatest', 'influence', 'the', 'republic', 'of', 'plato', 'is', 'also', 'the', 'first', 'treatise', 'upon', 'education', 'of', 'which', 'the', 'writings', 'of', 'milton', 'and', 'locke', 'rousseau', 'jean', 'paul', 'and', 'goethe', 'are', 'the', 'legitimate', 'descendants', 'like', 'dante', 'or', 'bunyan', 'he', 'has', 'a', 'revelation', 'of', 'another', 'life', 'like', 'bacon', 'he', 'is', 'profoundly', 'impressed', 'with', 'the', 'unity', 'of', 'knowledge', 'in', 'the', 'early', 'church', 'he', 'exercised', 'a', 'real', 'influence', 'on', 'theology', 'and', 'at', 'the', 'revival', 'of', 'literature', 'on', 'politics', 'even', 'the', 'fragments', 'of', 'his', 'words', 'when', 'at', 'symp', 'd', 'have', 'in', 'all', 'ages', 'ravished', 'the', 'hearts', 'of', 'men', 'who', 'have', 'seen', 'reflected', 'in', 'them', 'their', 'own', 'higher', 'nature', 'he', 'is', 'the', 'father', 'of', 'idealism', 'in', 'philosophy', 'in', 'politics', 'in', 'literature', 'and', 'many', 'of', 'the', 'latest', 'conceptions', 'of', 'modern', 'thinkers', 'and', 'statesmen', 'such', 'as', 'the', 'unity', 'of', 'knowledge', 'the', 'reign', 'of', 'law', 'and', 'the', 'equality', 'of', 'the', 'sexes', 'have', 'been', 'anticipated', 'in', 'a', 'dream', 'by', 'him', 'the', 'argument', 'of', 'the', 'republic', 'is', 'the', 'search', 'after', 'justice', 'the', 'nature', 'of', 'which', 'is', 'first', 'hinted', 'at', 'by', 'cephalus', 'the', 'just', 'and', 'blameless', 'old', 'discussed', 'on', 'the', 'basis', 'of', 'proverbial', 'morality', 'by', 'socrates', 'and', 'caricatured', 'by', 'thrasymachus', 'and', 'partially', 'explained', 'by', 'to', 'an', 'abstraction', 'by', 'glaucon', 'and', 'adeimantus', 'and', 'having', 'become', 'invisible', 'in', 'the', 'individual', 'reappears', 'at', 'length', 'in', 'the', 'ideal', 'state', 'which', 'is', 'constructed', 'by', 'socrates', 'the', 'first', 'care', 'of', 'the', 'rulers', 'is', 'to', 'be', 'education', 'of', 'which', 'an', 'outline', 'is', 'drawn', 'after', 'the', 'old', 'hellenic', 'model', 'providing', 'only', 'for', 'an', 'improved', 'religion', 'and', 'morality', 'and', 'more', 'simplicity', 'in', 'music', 'and', 'gymnastic', 'a', 'manlier', 'strain', 'of', 'poetry', 'and', 'greater', 'harmony', 'of', 'the', 'individual', 'and', 'the', 'state', 'we', 'are', 'thus', 'led', 'on', 'to', 'the', 'conception', 'of', 'a', 'higher', 'state', 'in', 'which', 'man', 'calls', 'anything', 'his', 'and', 'in', 'which', 'there', 'is', 'neither', 'nor', 'giving', 'in', 'and', 'are', 'and', 'are', 'and', 'there', 'is', 'another', 'and', 'higher', 'education', 'intellectual', 'as', 'well', 'as', 'moral', 'and', 'religious', 'of', 'science', 'as', 'well', 'as', 'of', 'art', 'and', 'not', 'of', 'youth', 'only', 'but', 'of', 'the', 'whole', 'of', 'life', 'such', 'a', 'state', 'is', 'hardly', 'to', 'be', 'realized', 'in', 'this', 'world', 'and', 'quickly', 'degenerates', 'to', 'the', 'perfect', 'ideal', 'succeeds', 'the', 'government', 'of', 'the', 'soldier', 'and', 'the', 'lover', 'of', 'honour', 'this', 'again', 'declining', 'into', 'democracy', 'and', 'democracy', 'into', 'tyranny', 'in', 'an', 'imaginary', 'but', 'regular', 'order', 'having', 'not', 'much', 'resemblance', 'to', 'the', 'actual', 'facts', 'when', 'wheel', 'has', 'come', 'full', 'we', 'do', 'not', 'begin', 'again', 'with', 'a', 'new', 'period', 'of', 'human', 'life', 'but', 'we', 'have', 'passed', 'from', 'the', 'best', 'to', 'the', 'worst', 'and', 'there', 'we', 'end', 'the', 'subject', 'is', 'then', 'changed', 'and', 'the', 'old', 'quarrel', 'of', 'poetry', 'and', 'philosophy', 'which', 'had', 'been', 'more', 'lightly', 'treated', 'in', 'the', 'earlier', 'books', 'of', 'the', 'republic', 'is', 'now', 'resumed', 'and', 'fought', 'out', 'to', 'a', 'conclusion', 'poetry', 'is', 'discovered', 'to', 'be', 'an', 'imitation', 'thrice', 'removed', 'from', 'the', 'truth', 'and', 'homer', 'as', 'well', 'as', 'the', 'dramatic', 'poets', 'having', 'been', 'condemned', 'as', 'an', 'imitator', 'is', 'sent', 'into', 'banishment', 'along', 'with', 'them', 'and', 'the', 'idea', 'of', 'the', 'state', 'is', 'supplemented', 'by', 'the', 'revelation', 'of', 'a', 'future', 'life', 'the', 'division', 'into', 'books', 'like', 'all', 'similar', 'divisions', 'cp', 'sir', 'gc', 'lewis', 'in', 'the', 'classical', 'museum', 'vol', 'ii', 'p', 'is', 'probably', 'later', 'than', 'the', 'age', 'of', 'plato', 'the', 'natural', 'divisions', 'are', 'five', 'in', 'book', 'i', 'and', 'the', 'first', 'half', 'of', 'book', 'ii', 'down', 'to', 'the', 'paragraph', 'beginning', 'had', 'always', 'admired', 'the', 'genius', 'of', 'glaucon', 'and', 'which', 'is', 'introductory', 'the', 'first', 'book', 'containing', 'a', 'refutation', 'of', 'the', 'popular', 'and', 'sophistical', 'notions', 'of', 'justice', 'and', 'concluding', 'like', 'some', 'of', 'the', 'earlier', 'dialogues', 'without', 'arriving', 'at', 'any', 'definite', 'result', 'to', 'this', 'is', 'appended', 'a', 'restatement', 'of', 'the', 'nature', 'of', 'justice', 'according', 'to', 'common', 'opinion', 'and', 'an', 'answer', 'is', 'demanded', 'to', 'the', 'is', 'justice', 'stripped', 'of', 'appearances', 'the', 'second', 'division', 'includes', 'the', 'remainder', 'of', 'the', 'second', 'and', 'the', 'whole', 'of', 'the', 'third', 'and', 'fourth', 'books', 'which', 'are', 'mainly', 'occupied', 'with', 'the', 'construction', 'of', 'the', 'first', 'state', 'and', 'the', 'first', 'education', 'the', 'third', 'division', 'consists', 'of', 'the', 'fifth', 'sixth', 'and', 'seventh', 'books', 'in', 'which', 'philosophy', 'rather', 'than', 'justice', 'is', 'the', 'subject', 'of', 'enquiry', 'and', 'the', 'second', 'state', 'is', 'constructed', 'on', 'principles', 'of', 'communism', 'and', 'ruled', 'by', 'philosophers', 'and', 'the', 'contemplation', 'of', 'the', 'idea', 'of', 'good', 'takes', 'the', 'place', 'of', 'the', 'social', 'and', 'political', 'virtues', 'in', 'the', 'eighth', 'and', 'ninth', 'books', 'the', 'perversions', 'of', 'states', 'and', 'of', 'the', 'individuals', 'who', 'correspond', 'to', 'them', 'are', 'reviewed', 'in', 'succession', 'and', 'the', 'nature', 'of', 'pleasure', 'and', 'the', 'principle', 'of', 'tyranny', 'are', 'further', 'analysed', 'in', 'the', 'individual', 'man', 'the', 'tenth', 'book', 'is', 'the', 'conclusion', 'of', 'the', 'whole', 'in', 'which', 'the', 'relations', 'of', 'philosophy', 'to', 'poetry', 'are', 'finally', 'determined', 'and', 'the', 'happiness', 'of', 'the', 'citizens', 'in', 'this', 'life', 'which', 'has', 'now', 'been', 'assured', 'is', 'crowned', 'by', 'the', 'vision', 'of', 'another', 'or', 'a', 'more', 'general', 'division', 'into', 'two', 'parts', 'may', 'be', 'adopted', 'the', 'first', 'books', 'i', 'iv', 'containing', 'the', 'description', 'of', 'a', 'state', 'framed', 'generally', 'in', 'accordance', 'with', 'hellenic', 'notions', 'of', 'religion', 'and', 'morality', 'while', 'in', 'the', 'second', 'books', 'v', 'x', 'the', 'hellenic', 'state', 'is', 'transformed', 'into', 'an', 'ideal', 'kingdom', 'of', 'philosophy', 'of', 'which']\n","Total Tokens: 216374\n","Unique Tokens: 10487\n","Total Sequences: 216245\n"]}],"source":["import string\n","import re\n","import keras\n","import tensorflow as tf\n","import numpy\n","from numpy import array\n","\n","def EntirePreprocessingOfTextForLSTMModels(path_of_dataset, num_train_sequence_length):\n","\n","\t# load doc into memory\n","\tdef load_doc(filename):\n","\t\t# open the file as read only\n","\t\tfile = open(filename, 'r')\n","\t\t# read all text\n","\t\ttext = file.read()\n","\t\t# close the file\n","\t\tfile.close()\n","\t\treturn text\n","\n","\n","\t# load document\n","\tin_filename = path_of_dataset\n","\tdoc = load_doc(in_filename)\n","\tprint(doc[:2000])\n","\n","\n","\tdef clean_text(text):\n","\t\t# text = text.lower()\n","\t\ttext = re.sub('\\[.*?\\]', '', text)\n","\t\ttext = re.sub('https?://\\S+|www\\.\\S+', '', text)\n","\t\ttext = re.sub('<.*?>+', '', text)\n","\t\ttext = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n","\t\ttext = re.sub('\\n', '', text)\n","\t\ttext = re.sub('\\w*\\d\\w*', '', text)\n","\t\treturn text\n","\n","\tdoc_final = clean_text(doc)\n","\tprint(type(doc_final))\n","\n","\n","\t# turn a doc into clean tokens\n","\tdef clean_doc(doc):\n","\t\t# replace '--' with a space ' '\n","\t\tdoc = doc.replace('--', ' ')\n","\t\t# split into tokens by white space\n","\t\ttokens = doc.split()\n","\t\t# remove punctuation from each token\n","\t\ttable = str.maketrans('', '', string.punctuation)\n","\t\ttokens = [w.translate(table) for w in tokens]\n","\t\t# remove remaining tokens that are not alphabetic\n","\t\ttokens = [word for word in tokens if word.isalpha()]\n","\t\t# make lower case\n","\t\ttokens = [word.lower() for word in tokens]\n","\t\treturn tokens\n","\n","\t# clean document\n","\ttokens = clean_doc(doc)\n","\tprint(tokens[:2000])\n","\n","\tprint('Total Tokens: %d' % len(tokens))\n","\tprint('Unique Tokens: %d' % len(set(tokens)))\n","\n","\n","\t# organize into sequences of tokens\n","\tlength = num_train_sequence_length + 1\n","\tsequences = list()\n","\tfor i in range(length, len(tokens)):\n","\t\t# select sequence of tokens\n","\t\tseq = tokens[i-length:i]\n","\t\t# convert into a line\n","\t\tline = ' '.join(seq)\n","\t\t# store\n","\t\tsequences.append(line)\n","\tprint('Total Sequences: %d' % len(sequences))\n","\n","\n","\t# save tokens to file, one dialog per line\n","\tdef save_doc(lines, filename):\n","\t\tdata = '\\n'.join(lines)\n","\t\tfile = open(filename, 'w')\n","\t\tfile.write(data)\n","\t\tfile.close()\n","\t\n","\t# save sequences to file\n","\tout_filename = './republic_sequences.txt'\n","\tsave_doc(sequences, out_filename)\n","\n","\n","\t# load doc into memory\n","\tdef load_doc(filename):\n","\t\t# open the file as read only\n","\t\tfile = open(filename, 'r')\n","\t\t# read all text\n","\t\ttext = file.read()\n","\t\t# close the file\n","\t\tfile.close()\n","\t\treturn text\n","\n","\t# load\n","\tin_filename = '/content/republic_sequences.txt'\n","\tdoc = load_doc(in_filename)\n","\tlines = doc.split('\\n')\n","\n","\n","\t# integer encode sequences of words\n","\ttokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=None,\n","\t\t\t\t\t\t\t\t\t\t\t\t\tfilters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n","\t\t\t\t\t\t\t\t\t\t\t\t\tlower=True,\n","\t\t\t\t\t\t\t\t\t\t\t\t\tsplit=' ')\n","\n","\ttokenizer.fit_on_texts(lines)\n","\tsequences = tokenizer.texts_to_sequences(lines)\n","\n","\t# vocabulary size\n","\tvocab_size = len(tokenizer.word_index) + 1\n","\n","\t# separate into input and output\n","\tsequences = array(sequences)\n","\tX, y = sequences[:,:-1], sequences[:,-1]\n","\ty = tf.keras.utils.to_categorical(y, num_classes=vocab_size)\n","\tseq_length = X.shape[1]\n","\n","\treturn X, y, seq_length, vocab_size\n","\n","\n","def ResidualBidirectionalCuDNNLSTM(inputtokens, vocabsize, layers, units, dropout, dropout_embedding, embedding):\n","\n","    input_ = tf.keras.layers.Input(shape=(inputtokens,), dtype='int32')\n","        \n","    # Embedding layer\n","    net = tf.keras.layers.Embedding(input_dim=vocabsize, output_dim=embedding, input_length=inputtokens)(input_)\n","    net = tf.keras.layers.Dropout(dropout_embedding)(net)\n","            \n","    # Bidirectional LSTM layer\n","    net = tf.keras.layers.BatchNormalization()(net)\n","    net = tf.keras.layers.Bidirectional(tf.compat.v1.keras.layers.CuDNNLSTM(units, return_sequences=(layers > 1)))(net)\n","    net = tf.keras.layers.Dropout(dropout)(net)\n","            \n","    # Rest of LSTM layers with residual connections (if any)\n","    for i in range(1, layers):\n","        if i < layers-1:\n","            block = tf.keras.layers.BatchNormalization()(net)\n","            block = tf.compat.v1.keras.layers.CuDNNLSTM(2*units, return_sequences=True)(block)\n","            block = tf.keras.layers.Dropout(dropout)(block)\n","            net = tf.keras.layers.add([block, net])\n","        else:\n","            net = tf.keras.layers.BatchNormalization()(net)\n","            net = tf.compat.v1.keras.layers.CuDNNLSTM(2*units)(net)\n","            net = tf.keras.layers.Dropout(dropout)(net)\n","                    \n","    # Output layer\n","    net = tf.keras.layers.Dense(vocabsize, activation='softmax')(net)\n","    model = tf.keras.Model(inputs=input_, outputs=net)\n","\n","    return model \n","\n","\n","X, y, sequence_length, vocabulary_size = EntirePreprocessingOfTextForLSTMModels(path_of_dataset='/content/drive/MyDrive/Twitter-Sentiment-Analysis/Word-Level-Text-Generation/Plato.txt', \n","                                                                \t\t\t\tnum_train_sequence_length=128)\n","\n","\n","model = ResidualBidirectionalCuDNNLSTM(128, vocabulary_size, 9, 16, 0.15, 0, 32)\n","\n"]},{"cell_type":"code","source":["print(\"The shape of dataset is : {}\".format(X.shape))\n","print(\"The shape of targets is : {}\".format(y.shape))"],"metadata":{"id":"ckvDORWOSbFE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650439481718,"user_tz":-330,"elapsed":697,"user":{"displayName":"VR AI IEMA","userId":"08782298689493191086"}},"outputId":"190247c7-d6fe-490b-b39d-9096fa945040"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["The shape of dataset is : (216245, 128)\n","The shape of targets is : (216245, 10488)\n"]}]},{"cell_type":"code","source":["model.summary()"],"metadata":{"id":"xMhFLA2Jy6Gu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650439488611,"user_tz":-330,"elapsed":875,"user":{"displayName":"VR AI IEMA","userId":"08782298689493191086"}},"outputId":"1fc50440-2dac-491d-9733-226eb5ab6ca8"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 128)]        0           []                               \n","                                                                                                  \n"," embedding (Embedding)          (None, 128, 32)      335616      ['input_1[0][0]']                \n","                                                                                                  \n"," dropout (Dropout)              (None, 128, 32)      0           ['embedding[0][0]']              \n","                                                                                                  \n"," batch_normalization (BatchNorm  (None, 128, 32)     128         ['dropout[0][0]']                \n"," alization)                                                                                       \n","                                                                                                  \n"," bidirectional (Bidirectional)  (None, 128, 32)      6400        ['batch_normalization[0][0]']    \n","                                                                                                  \n"," dropout_1 (Dropout)            (None, 128, 32)      0           ['bidirectional[0][0]']          \n","                                                                                                  \n"," batch_normalization_1 (BatchNo  (None, 128, 32)     128         ['dropout_1[0][0]']              \n"," rmalization)                                                                                     \n","                                                                                                  \n"," cu_dnnlstm_1 (CuDNNLSTM)       (None, 128, 32)      8448        ['batch_normalization_1[0][0]']  \n","                                                                                                  \n"," dropout_2 (Dropout)            (None, 128, 32)      0           ['cu_dnnlstm_1[0][0]']           \n","                                                                                                  \n"," add (Add)                      (None, 128, 32)      0           ['dropout_2[0][0]',              \n","                                                                  'dropout_1[0][0]']              \n","                                                                                                  \n"," batch_normalization_2 (BatchNo  (None, 128, 32)     128         ['add[0][0]']                    \n"," rmalization)                                                                                     \n","                                                                                                  \n"," cu_dnnlstm_2 (CuDNNLSTM)       (None, 128, 32)      8448        ['batch_normalization_2[0][0]']  \n","                                                                                                  \n"," dropout_3 (Dropout)            (None, 128, 32)      0           ['cu_dnnlstm_2[0][0]']           \n","                                                                                                  \n"," add_1 (Add)                    (None, 128, 32)      0           ['dropout_3[0][0]',              \n","                                                                  'add[0][0]']                    \n","                                                                                                  \n"," batch_normalization_3 (BatchNo  (None, 128, 32)     128         ['add_1[0][0]']                  \n"," rmalization)                                                                                     \n","                                                                                                  \n"," cu_dnnlstm_3 (CuDNNLSTM)       (None, 128, 32)      8448        ['batch_normalization_3[0][0]']  \n","                                                                                                  \n"," dropout_4 (Dropout)            (None, 128, 32)      0           ['cu_dnnlstm_3[0][0]']           \n","                                                                                                  \n"," add_2 (Add)                    (None, 128, 32)      0           ['dropout_4[0][0]',              \n","                                                                  'add_1[0][0]']                  \n","                                                                                                  \n"," batch_normalization_4 (BatchNo  (None, 128, 32)     128         ['add_2[0][0]']                  \n"," rmalization)                                                                                     \n","                                                                                                  \n"," cu_dnnlstm_4 (CuDNNLSTM)       (None, 128, 32)      8448        ['batch_normalization_4[0][0]']  \n","                                                                                                  \n"," dropout_5 (Dropout)            (None, 128, 32)      0           ['cu_dnnlstm_4[0][0]']           \n","                                                                                                  \n"," add_3 (Add)                    (None, 128, 32)      0           ['dropout_5[0][0]',              \n","                                                                  'add_2[0][0]']                  \n","                                                                                                  \n"," batch_normalization_5 (BatchNo  (None, 128, 32)     128         ['add_3[0][0]']                  \n"," rmalization)                                                                                     \n","                                                                                                  \n"," cu_dnnlstm_5 (CuDNNLSTM)       (None, 128, 32)      8448        ['batch_normalization_5[0][0]']  \n","                                                                                                  \n"," dropout_6 (Dropout)            (None, 128, 32)      0           ['cu_dnnlstm_5[0][0]']           \n","                                                                                                  \n"," add_4 (Add)                    (None, 128, 32)      0           ['dropout_6[0][0]',              \n","                                                                  'add_3[0][0]']                  \n","                                                                                                  \n"," batch_normalization_6 (BatchNo  (None, 128, 32)     128         ['add_4[0][0]']                  \n"," rmalization)                                                                                     \n","                                                                                                  \n"," cu_dnnlstm_6 (CuDNNLSTM)       (None, 128, 32)      8448        ['batch_normalization_6[0][0]']  \n","                                                                                                  \n"," dropout_7 (Dropout)            (None, 128, 32)      0           ['cu_dnnlstm_6[0][0]']           \n","                                                                                                  \n"," add_5 (Add)                    (None, 128, 32)      0           ['dropout_7[0][0]',              \n","                                                                  'add_4[0][0]']                  \n","                                                                                                  \n"," batch_normalization_7 (BatchNo  (None, 128, 32)     128         ['add_5[0][0]']                  \n"," rmalization)                                                                                     \n","                                                                                                  \n"," cu_dnnlstm_7 (CuDNNLSTM)       (None, 128, 32)      8448        ['batch_normalization_7[0][0]']  \n","                                                                                                  \n"," dropout_8 (Dropout)            (None, 128, 32)      0           ['cu_dnnlstm_7[0][0]']           \n","                                                                                                  \n"," add_6 (Add)                    (None, 128, 32)      0           ['dropout_8[0][0]',              \n","                                                                  'add_5[0][0]']                  \n","                                                                                                  \n"," batch_normalization_8 (BatchNo  (None, 128, 32)     128         ['add_6[0][0]']                  \n"," rmalization)                                                                                     \n","                                                                                                  \n"," cu_dnnlstm_8 (CuDNNLSTM)       (None, 32)           8448        ['batch_normalization_8[0][0]']  \n","                                                                                                  \n"," dropout_9 (Dropout)            (None, 32)           0           ['cu_dnnlstm_8[0][0]']           \n","                                                                                                  \n"," dense (Dense)                  (None, 10488)        346104      ['dropout_9[0][0]']              \n","                                                                                                  \n","==================================================================================================\n","Total params: 756,856\n","Trainable params: 756,280\n","Non-trainable params: 576\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","source":["import re  \n","import string\n","\n","def clean_text(text):\n","    # text = text.lower()\n","    text = re.sub('\\[.*?\\]', '', text)\n","    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n","    text = re.sub('<.*?>+', '', text)\n","    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n","    text = re.sub('\\n', '', text)\n","    text = re.sub('\\w*\\d\\w*', '', text)\n","    return text\n","\n","doc_final = clean_text(doc)\n","print(type(doc_final))"],"metadata":{"id":"H4xc4bC1BgvV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import string\n","\n","# turn a doc into clean tokens\n","def clean_doc(doc):\n","\t# replace '--' with a space ' '\n","\tdoc = doc.replace('--', ' ')\n","\t# split into tokens by white space\n","\ttokens = doc.split()\n","\t# remove punctuation from each token\n","\ttable = str.maketrans('', '', string.punctuation)\n","\ttokens = [w.translate(table) for w in tokens]\n","\t# remove remaining tokens that are not alphabetic\n","\ttokens = [word for word in tokens if word.isalpha()]\n","\t# make lower case\n","\ttokens = [word.lower() for word in tokens]\n","\treturn tokens\n","\n","\n","# clean document\n","tokens = clean_doc(doc)\n","print(tokens[:2000])\n","\n","print('Total Tokens: %d' % len(tokens))\n","print('Unique Tokens: %d' % len(set(tokens)))"],"metadata":{"id":"UpZ0yuY2hSQM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# organize into sequences of tokens\n","length = 175 + 1\n","sequences = list()\n","for i in range(length, len(tokens)):\n","\t# select sequence of tokens\n","\tseq = tokens[i-length:i]\n","\t# convert into a line\n","\tline = ' '.join(seq)\n","\t# store\n","\tsequences.append(line)\n","print('Total Sequences: %d' % len(sequences))\n","\n"],"metadata":{"id":"oPRvweEdheMa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save tokens to file, one dialog per line\n","def save_doc(lines, filename):\n","\tdata = '\\n'.join(lines)\n","\tfile = open(filename, 'w')\n","\tfile.write(data)\n","\tfile.close()\n"," \n","# save sequences to file\n","out_filename = './republic_sequences.txt'\n","save_doc(sequences, out_filename)\n","\n"],"metadata":{"id":"0JmZng-RhrQs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load doc into memory\n","def load_doc(filename):\n","\t# open the file as read only\n","\tfile = open(filename, 'r')\n","\t# read all text\n","\ttext = file.read()\n","\t# close the file\n","\tfile.close()\n","\treturn text\n","\n","# load\n","in_filename = '/content/republic_sequences.txt'\n","doc = load_doc(in_filename)\n","lines = doc.split('\\n')\n","\n"],"metadata":{"id":"iPfHM-dbhzhT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","import keras"],"metadata":{"id":"JpAPfl-9iWOT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# integer encode sequences of words\n","tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=None,\n","                                                  filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n","                                                  lower=True,\n","                                                  split=' ')\n","\n","tokenizer.fit_on_texts(lines)\n","sequences = tokenizer.texts_to_sequences(lines)"],"metadata":{"id":"zb4TtEvyiAF7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(sequences)"],"metadata":{"id":"UQYuNvvSih7V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# vocabulary size\n","vocab_size = len(tokenizer.word_index) + 1"],"metadata":{"id":"RhdMXBkViood"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Vocabulary Size is : {}\".format(vocab_size))"],"metadata":{"id":"SWDwUCJxitt7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow.keras as keras\n","import tensorflow as tf\n"],"metadata":{"id":"0yndCYt4k6nz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from numpy import array\n","from pickle import dump\n","from keras.preprocessing.text import Tokenizer\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.layers import Embedding\n","\n","\n","# define model\n","model = Sequential()\n","\n","model.add(Embedding(vocabulary_size, 50, input_length=sequence_length))\n","\n","model.add(tf.compat.v1.keras.layers.CuDNNLSTM(175, return_sequences=False))\n","model.add(tf.keras.layers.BatchNormalization(axis=-1,\n","                                            momentum=0.99,\n","\t\t\t\t\t\t\t\t\t\t\tepsilon=0.001,))\n","model.add(tf.keras.layers.Dropout(rate=0.10))\n","\n","model.add(Dense(125, activation='relu'))\n","model.add(tf.keras.layers.BatchNormalization(axis=-1,\n","                                            momentum=0.99,\n","\t\t\t\t\t\t\t\t\t\t\tepsilon=0.001,))\n","model.add(tf.keras.layers.Dropout(rate=0.20))\n","\n","model.add(Dense(75, activation='relu'))\n","model.add(tf.keras.layers.BatchNormalization(axis=-1,\n","                                            momentum=0.99,\n","\t\t\t\t\t\t\t\t\t\t\tepsilon=0.001,))\n","model.add(tf.keras.layers.Dropout(rate=0.10))\n","\n","model.add(Dense(vocabulary_size, activation='softmax'))\n","\n","print(model.summary())\n","# compile model\n","\n","\n","model.compile(loss='categorical_crossentropy', \n","              optimizer='adam', \n","              metrics=['accuracy'])\n","# fit model\n","\n","\n","callback_1 = tf.keras.callbacks.EarlyStopping(monitor='loss', \n","                                            patience=15)\n","\n","reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n","                                                factor=0.2,\n","\t\t\t\t\t\t\t\t\t\t\t\tpatience=5,\n","\t\t\t\t\t\t\t\t\t\t\t\tmin_lr=0.001)\n","\n","history = model.fit(X, y, batch_size=256, \n","                    epochs=100)\n","\n"],"metadata":{"id":"ReDr32mDizib"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=None,\n","                                                  filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n","                                                  lower=True,\n","                                                  split=' ')\n","# save the model to file\n","model.save('./model-cudnnlstm.h5')\n","# save the tokenizer\n","dump(tokenizer, open('tokenizer.pkl', 'wb'))"],"metadata":{"id":"u2yyeWHVjFcr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from random import randint\n","from pickle import load\n","from keras.models import load_model\n","from keras.preprocessing.sequence import pad_sequences\n","import numpy as np\n","\n","# load doc into memory\n","def load_doc(filename):\n","\t# open the file as read only\n","\tfile = open(filename, 'r')\n","\t# read all text\n","\ttext = file.read()\n","\t# close the file\n","\tfile.close()\n","\treturn text\n","\n","# generate a sequence from a language model\n","def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n","\tresult = list()\n","\tin_text = seed_text\n","\t# generate a fixed number of words\n","\tfor _ in range(n_words):\n","\t\t# encode the text as integer\n","\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n","\t\t# truncate sequences to a fixed length\n","\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n","\t\t# predict probabilities for each word\n","\t\tyhat = np.argmax(model.predict(encoded), axis=-1)\n","\t\t# map predicted word index to word\n","\t\tout_word = ''\n","\t\tfor word, index in tokenizer.word_index.items():\n","\t\t\tif index == yhat:\n","\t\t\t\tout_word = word\n","\t\t\t\tbreak\n","\t\t# append to input\n","\t\tin_text += ' ' + out_word\n","\t\tresult.append(out_word)\n","\treturn ' '.join(result)\n","\n","# load cleaned text sequences\n","in_filename = 'republic_sequences.txt'\n","doc = load_doc(in_filename)\n","lines = doc.split('\\n')\n","seq_length = len(lines[0].split()) - 1\n","\n","# load the model\n","model = load_model('/content/model-cudnnlstm.h5')\n","\n","# load the tokenizer\n","tokenizer = load(open('/content/tokenizer.pkl', 'rb'))\n","\n","# select a seed text\n","seed_text = lines[randint(0,len(lines))]\n","print(seed_text + '\\n')\n","\n","# generate new text\n","generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n","print(generated)\n"],"metadata":{"id":"oILFkaqzmnKt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def ResidualBidirectionalCuDNNLSTM(inputtokens, vocabsize, layers, units, dropout, embedding):\n","\n","    input_ = tf.keras.layers.Input(shape=(inputtokens,), dtype='int32')\n","        \n","    # Embedding layer\n","    net = tf.keras.layers.Embedding(input_dim=vocabsize, output_dim=embedding, input_length=inputtokens)(input_)\n","    net = tf.keras.layers.Dropout(dropout)(net)\n","            \n","    # Bidirectional LSTM layer\n","    net = tf.keras.layers.BatchNormalization()(net)\n","    net = tf.keras.layers.Bidirectional(tf.compat.v1.keras.layers.CuDNNLSTM(units, return_sequences=(layers > 1)))(net)\n","    net = tf.keras.layers.Dropout(dropout)(net)\n","            \n","    # Rest of LSTM layers with residual connections (if any)\n","    for i in range(1, layers):\n","        if i < layers-1:\n","            block = tf.keras.layers.BatchNormalization()(net)\n","            block = tf.compat.v1.keras.layers.CuDNNLSTM(2*units, return_sequences=True)(block)\n","            block = tf.keras.layers.Dropout(dropout)(block)\n","            net = tf.keras.layers.add([block, net])\n","        else:\n","            net = tf.keras.layers.BatchNormalization()(net)\n","            net = tf.compat.v1.keras.layers.CuDNNLSTM(2*units)(net)\n","            net = tf.keras.layers.Dropout(dropout)(net)\n","                    \n","    # Output layer\n","    net = tf.keras.layers.Dense(vocabsize, activation='softmax')(net)\n","    model = tf.keras.Model(inputs=input_, outputs=net)\n","\n","    return model \n","\n","model = ResidualBidirectionalCuDNNLSTM(50, vocabulary_size, 1, 64, 0, 128)"],"metadata":{"id":"YjBqZ8nznAlT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"id":"wZFHZgNPMWeK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650436276251,"user_tz":-330,"elapsed":686,"user":{"displayName":"VR AI IEMA","userId":"08782298689493191086"}},"outputId":"e9b3ae7a-b409-4e26-f690-744c43f7d66b"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 128)]        0           []                               \n","                                                                                                  \n"," embedding (Embedding)          (None, 128, 32)      335616      ['input_1[0][0]']                \n","                                                                                                  \n"," dropout (Dropout)              (None, 128, 32)      0           ['embedding[0][0]']              \n","                                                                                                  \n"," batch_normalization (BatchNorm  (None, 128, 32)     128         ['dropout[0][0]']                \n"," alization)                                                                                       \n","                                                                                                  \n"," bidirectional (Bidirectional)  (None, 128, 32)      6400        ['batch_normalization[0][0]']    \n","                                                                                                  \n"," dropout_1 (Dropout)            (None, 128, 32)      0           ['bidirectional[0][0]']          \n","                                                                                                  \n"," batch_normalization_1 (BatchNo  (None, 128, 32)     128         ['dropout_1[0][0]']              \n"," rmalization)                                                                                     \n","                                                                                                  \n"," cu_dnnlstm_1 (CuDNNLSTM)       (None, 128, 32)      8448        ['batch_normalization_1[0][0]']  \n","                                                                                                  \n"," dropout_2 (Dropout)            (None, 128, 32)      0           ['cu_dnnlstm_1[0][0]']           \n","                                                                                                  \n"," add (Add)                      (None, 128, 32)      0           ['dropout_2[0][0]',              \n","                                                                  'dropout_1[0][0]']              \n","                                                                                                  \n"," batch_normalization_2 (BatchNo  (None, 128, 32)     128         ['add[0][0]']                    \n"," rmalization)                                                                                     \n","                                                                                                  \n"," cu_dnnlstm_2 (CuDNNLSTM)       (None, 128, 32)      8448        ['batch_normalization_2[0][0]']  \n","                                                                                                  \n"," dropout_3 (Dropout)            (None, 128, 32)      0           ['cu_dnnlstm_2[0][0]']           \n","                                                                                                  \n"," add_1 (Add)                    (None, 128, 32)      0           ['dropout_3[0][0]',              \n","                                                                  'add[0][0]']                    \n","                                                                                                  \n"," batch_normalization_3 (BatchNo  (None, 128, 32)     128         ['add_1[0][0]']                  \n"," rmalization)                                                                                     \n","                                                                                                  \n"," cu_dnnlstm_3 (CuDNNLSTM)       (None, 128, 32)      8448        ['batch_normalization_3[0][0]']  \n","                                                                                                  \n"," dropout_4 (Dropout)            (None, 128, 32)      0           ['cu_dnnlstm_3[0][0]']           \n","                                                                                                  \n"," add_2 (Add)                    (None, 128, 32)      0           ['dropout_4[0][0]',              \n","                                                                  'add_1[0][0]']                  \n","                                                                                                  \n"," batch_normalization_4 (BatchNo  (None, 128, 32)     128         ['add_2[0][0]']                  \n"," rmalization)                                                                                     \n","                                                                                                  \n"," cu_dnnlstm_4 (CuDNNLSTM)       (None, 128, 32)      8448        ['batch_normalization_4[0][0]']  \n","                                                                                                  \n"," dropout_5 (Dropout)            (None, 128, 32)      0           ['cu_dnnlstm_4[0][0]']           \n","                                                                                                  \n"," add_3 (Add)                    (None, 128, 32)      0           ['dropout_5[0][0]',              \n","                                                                  'add_2[0][0]']                  \n","                                                                                                  \n"," batch_normalization_5 (BatchNo  (None, 128, 32)     128         ['add_3[0][0]']                  \n"," rmalization)                                                                                     \n","                                                                                                  \n"," cu_dnnlstm_5 (CuDNNLSTM)       (None, 128, 32)      8448        ['batch_normalization_5[0][0]']  \n","                                                                                                  \n"," dropout_6 (Dropout)            (None, 128, 32)      0           ['cu_dnnlstm_5[0][0]']           \n","                                                                                                  \n"," add_4 (Add)                    (None, 128, 32)      0           ['dropout_6[0][0]',              \n","                                                                  'add_3[0][0]']                  \n","                                                                                                  \n"," batch_normalization_6 (BatchNo  (None, 128, 32)     128         ['add_4[0][0]']                  \n"," rmalization)                                                                                     \n","                                                                                                  \n"," cu_dnnlstm_6 (CuDNNLSTM)       (None, 128, 32)      8448        ['batch_normalization_6[0][0]']  \n","                                                                                                  \n"," dropout_7 (Dropout)            (None, 128, 32)      0           ['cu_dnnlstm_6[0][0]']           \n","                                                                                                  \n"," add_5 (Add)                    (None, 128, 32)      0           ['dropout_7[0][0]',              \n","                                                                  'add_4[0][0]']                  \n","                                                                                                  \n"," batch_normalization_7 (BatchNo  (None, 128, 32)     128         ['add_5[0][0]']                  \n"," rmalization)                                                                                     \n","                                                                                                  \n"," cu_dnnlstm_7 (CuDNNLSTM)       (None, 128, 32)      8448        ['batch_normalization_7[0][0]']  \n","                                                                                                  \n"," dropout_8 (Dropout)            (None, 128, 32)      0           ['cu_dnnlstm_7[0][0]']           \n","                                                                                                  \n"," add_6 (Add)                    (None, 128, 32)      0           ['dropout_8[0][0]',              \n","                                                                  'add_5[0][0]']                  \n","                                                                                                  \n"," batch_normalization_8 (BatchNo  (None, 128, 32)     128         ['add_6[0][0]']                  \n"," rmalization)                                                                                     \n","                                                                                                  \n"," cu_dnnlstm_8 (CuDNNLSTM)       (None, 32)           8448        ['batch_normalization_8[0][0]']  \n","                                                                                                  \n"," dropout_9 (Dropout)            (None, 32)           0           ['cu_dnnlstm_8[0][0]']           \n","                                                                                                  \n"," dense (Dense)                  (None, 10488)        346104      ['dropout_9[0][0]']              \n","                                                                                                  \n","==================================================================================================\n","Total params: 756,856\n","Trainable params: 756,280\n","Non-trainable params: 576\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","source":["\n","model.compile(loss='categorical_crossentropy', \n","              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n","              metrics=['accuracy'])\n","# fit model\n","\n","callback_1 = tf.keras.callbacks.EarlyStopping(monitor='loss', \n","                                            patience=15)\n","\n","reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', \n","                                                factor=0.2,\n","\t\t\t\t\t\t\t\t\t\t\t\tpatience=5,\n","\t\t\t\t\t\t\t\t\t\t\t\tmin_lr=0.001)\n","\n","history = model.fit(\n","    X, y, batch_size=256, \n","    epochs=100, \n","    callbacks=[callback_1, \n","                reduce_lr]\n","                    )\n"],"metadata":{"id":"jHiIWyLaMYrh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650445069078,"user_tz":-330,"elapsed":5277408,"user":{"displayName":"VR AI IEMA","userId":"08782298689493191086"}},"outputId":"75dabe1b-2f25-48bc-d41f-b29842c20981"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","845/845 [==============================] - 64s 62ms/step - loss: 6.4412 - accuracy: 0.0744 - lr: 0.0010\n","Epoch 2/100\n","845/845 [==============================] - 52s 62ms/step - loss: 5.9547 - accuracy: 0.1056 - lr: 0.0010\n","Epoch 3/100\n","845/845 [==============================] - 52s 62ms/step - loss: 5.7333 - accuracy: 0.1301 - lr: 0.0010\n","Epoch 4/100\n","845/845 [==============================] - 52s 62ms/step - loss: 5.5734 - accuracy: 0.1439 - lr: 0.0010\n","Epoch 5/100\n","845/845 [==============================] - 52s 62ms/step - loss: 5.4518 - accuracy: 0.1535 - lr: 0.0010\n","Epoch 6/100\n","845/845 [==============================] - 53s 62ms/step - loss: 5.3534 - accuracy: 0.1622 - lr: 0.0010\n","Epoch 7/100\n","845/845 [==============================] - 52s 62ms/step - loss: 5.2740 - accuracy: 0.1695 - lr: 0.0010\n","Epoch 8/100\n","845/845 [==============================] - 52s 62ms/step - loss: 5.2037 - accuracy: 0.1749 - lr: 0.0010\n","Epoch 9/100\n","845/845 [==============================] - 53s 62ms/step - loss: 5.1458 - accuracy: 0.1783 - lr: 0.0010\n","Epoch 10/100\n","845/845 [==============================] - 53s 62ms/step - loss: 5.0929 - accuracy: 0.1825 - lr: 0.0010\n","Epoch 11/100\n","845/845 [==============================] - 52s 62ms/step - loss: 5.0434 - accuracy: 0.1859 - lr: 0.0010\n","Epoch 12/100\n","845/845 [==============================] - 53s 62ms/step - loss: 5.0008 - accuracy: 0.1887 - lr: 0.0010\n","Epoch 13/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.9591 - accuracy: 0.1910 - lr: 0.0010\n","Epoch 14/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.9212 - accuracy: 0.1942 - lr: 0.0010\n","Epoch 15/100\n","845/845 [==============================] - 52s 62ms/step - loss: 4.8871 - accuracy: 0.1960 - lr: 0.0010\n","Epoch 16/100\n","845/845 [==============================] - 52s 62ms/step - loss: 4.8550 - accuracy: 0.1979 - lr: 0.0010\n","Epoch 17/100\n","845/845 [==============================] - 52s 62ms/step - loss: 4.8233 - accuracy: 0.2002 - lr: 0.0010\n","Epoch 18/100\n","845/845 [==============================] - 52s 62ms/step - loss: 4.7965 - accuracy: 0.2016 - lr: 0.0010\n","Epoch 19/100\n","845/845 [==============================] - 52s 62ms/step - loss: 4.7699 - accuracy: 0.2029 - lr: 0.0010\n","Epoch 20/100\n","845/845 [==============================] - 52s 62ms/step - loss: 4.7427 - accuracy: 0.2050 - lr: 0.0010\n","Epoch 21/100\n","845/845 [==============================] - 52s 62ms/step - loss: 4.7235 - accuracy: 0.2062 - lr: 0.0010\n","Epoch 22/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.7013 - accuracy: 0.2072 - lr: 0.0010\n","Epoch 23/100\n","845/845 [==============================] - 52s 62ms/step - loss: 4.6775 - accuracy: 0.2088 - lr: 0.0010\n","Epoch 24/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.6565 - accuracy: 0.2097 - lr: 0.0010\n","Epoch 25/100\n","845/845 [==============================] - 52s 62ms/step - loss: 4.6383 - accuracy: 0.2116 - lr: 0.0010\n","Epoch 26/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.6221 - accuracy: 0.2120 - lr: 0.0010\n","Epoch 27/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.6082 - accuracy: 0.2120 - lr: 0.0010\n","Epoch 28/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.5896 - accuracy: 0.2142 - lr: 0.0010\n","Epoch 29/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.5706 - accuracy: 0.2154 - lr: 0.0010\n","Epoch 30/100\n","845/845 [==============================] - 52s 62ms/step - loss: 4.5557 - accuracy: 0.2163 - lr: 0.0010\n","Epoch 31/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.5370 - accuracy: 0.2185 - lr: 0.0010\n","Epoch 32/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.5217 - accuracy: 0.2191 - lr: 0.0010\n","Epoch 33/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.5090 - accuracy: 0.2194 - lr: 0.0010\n","Epoch 34/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.4941 - accuracy: 0.2214 - lr: 0.0010\n","Epoch 35/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.4820 - accuracy: 0.2218 - lr: 0.0010\n","Epoch 36/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.4693 - accuracy: 0.2223 - lr: 0.0010\n","Epoch 37/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.4580 - accuracy: 0.2233 - lr: 0.0010\n","Epoch 38/100\n","845/845 [==============================] - 52s 62ms/step - loss: 4.4445 - accuracy: 0.2240 - lr: 0.0010\n","Epoch 39/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.4334 - accuracy: 0.2259 - lr: 0.0010\n","Epoch 40/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.4218 - accuracy: 0.2262 - lr: 0.0010\n","Epoch 41/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.4095 - accuracy: 0.2267 - lr: 0.0010\n","Epoch 42/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.4004 - accuracy: 0.2268 - lr: 0.0010\n","Epoch 43/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.3911 - accuracy: 0.2282 - lr: 0.0010\n","Epoch 44/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.3768 - accuracy: 0.2287 - lr: 0.0010\n","Epoch 45/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.3696 - accuracy: 0.2298 - lr: 0.0010\n","Epoch 46/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.3578 - accuracy: 0.2305 - lr: 0.0010\n","Epoch 47/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.3510 - accuracy: 0.2312 - lr: 0.0010\n","Epoch 48/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.3417 - accuracy: 0.2315 - lr: 0.0010\n","Epoch 49/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.3344 - accuracy: 0.2325 - lr: 0.0010\n","Epoch 50/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.3239 - accuracy: 0.2329 - lr: 0.0010\n","Epoch 51/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.3140 - accuracy: 0.2335 - lr: 0.0010\n","Epoch 52/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.3108 - accuracy: 0.2340 - lr: 0.0010\n","Epoch 53/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.3012 - accuracy: 0.2345 - lr: 0.0010\n","Epoch 54/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.2929 - accuracy: 0.2346 - lr: 0.0010\n","Epoch 55/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.2820 - accuracy: 0.2362 - lr: 0.0010\n","Epoch 56/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.2794 - accuracy: 0.2375 - lr: 0.0010\n","Epoch 57/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.2677 - accuracy: 0.2377 - lr: 0.0010\n","Epoch 58/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.2616 - accuracy: 0.2382 - lr: 0.0010\n","Epoch 59/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.2549 - accuracy: 0.2379 - lr: 0.0010\n","Epoch 60/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.2502 - accuracy: 0.2370 - lr: 0.0010\n","Epoch 61/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.2411 - accuracy: 0.2399 - lr: 0.0010\n","Epoch 62/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.2345 - accuracy: 0.2404 - lr: 0.0010\n","Epoch 63/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.2264 - accuracy: 0.2402 - lr: 0.0010\n","Epoch 64/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.2218 - accuracy: 0.2414 - lr: 0.0010\n","Epoch 65/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.2178 - accuracy: 0.2418 - lr: 0.0010\n","Epoch 66/100\n","845/845 [==============================] - 52s 62ms/step - loss: 4.2073 - accuracy: 0.2417 - lr: 0.0010\n","Epoch 67/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.2013 - accuracy: 0.2431 - lr: 0.0010\n","Epoch 68/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.1964 - accuracy: 0.2428 - lr: 0.0010\n","Epoch 69/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.1914 - accuracy: 0.2443 - lr: 0.0010\n","Epoch 70/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.1821 - accuracy: 0.2448 - lr: 0.0010\n","Epoch 71/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.1799 - accuracy: 0.2449 - lr: 0.0010\n","Epoch 72/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.1734 - accuracy: 0.2447 - lr: 0.0010\n","Epoch 73/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.1763 - accuracy: 0.2444 - lr: 0.0010\n","Epoch 74/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.1635 - accuracy: 0.2453 - lr: 0.0010\n","Epoch 75/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.1587 - accuracy: 0.2465 - lr: 0.0010\n","Epoch 76/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.1511 - accuracy: 0.2473 - lr: 0.0010\n","Epoch 77/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.1457 - accuracy: 0.2481 - lr: 0.0010\n","Epoch 78/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.1402 - accuracy: 0.2487 - lr: 0.0010\n","Epoch 79/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.1353 - accuracy: 0.2484 - lr: 0.0010\n","Epoch 80/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.1298 - accuracy: 0.2488 - lr: 0.0010\n","Epoch 81/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.1284 - accuracy: 0.2487 - lr: 0.0010\n","Epoch 82/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.1218 - accuracy: 0.2484 - lr: 0.0010\n","Epoch 83/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.1189 - accuracy: 0.2506 - lr: 0.0010\n","Epoch 84/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.1121 - accuracy: 0.2512 - lr: 0.0010\n","Epoch 85/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.1113 - accuracy: 0.2511 - lr: 0.0010\n","Epoch 86/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.1054 - accuracy: 0.2507 - lr: 0.0010\n","Epoch 87/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.1008 - accuracy: 0.2511 - lr: 0.0010\n","Epoch 88/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.0974 - accuracy: 0.2511 - lr: 0.0010\n","Epoch 89/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.0914 - accuracy: 0.2517 - lr: 0.0010\n","Epoch 90/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.0863 - accuracy: 0.2526 - lr: 0.0010\n","Epoch 91/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.0848 - accuracy: 0.2522 - lr: 0.0010\n","Epoch 92/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.0779 - accuracy: 0.2543 - lr: 0.0010\n","Epoch 93/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.0741 - accuracy: 0.2543 - lr: 0.0010\n","Epoch 94/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.0685 - accuracy: 0.2544 - lr: 0.0010\n","Epoch 95/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.0658 - accuracy: 0.2545 - lr: 0.0010\n","Epoch 96/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.0575 - accuracy: 0.2560 - lr: 0.0010\n","Epoch 97/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.0580 - accuracy: 0.2557 - lr: 0.0010\n","Epoch 98/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.0560 - accuracy: 0.2550 - lr: 0.0010\n","Epoch 99/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.0796 - accuracy: 0.2528 - lr: 0.0010\n","Epoch 100/100\n","845/845 [==============================] - 53s 62ms/step - loss: 4.0584 - accuracy: 0.2550 - lr: 0.0010\n"]}]},{"cell_type":"markdown","source":["#### The actual text generation code with Tensorflow"],"metadata":{"id":"1qZpHkk0_VLp"}},{"cell_type":"code","source":["from numpy import array\n","from pickle import dump\n","from keras.preprocessing.text import Tokenizer\n","tokenizer = Tokenizer()\n","\n","# save the model to file\n","model.save('model.h5')\n","# save the tokenizer\n","dump(tokenizer, open('tokenizer.pkl', 'wb'))"],"metadata":{"id":"hADguqKwZMxG","executionInfo":{"status":"ok","timestamp":1650448511404,"user_tz":-330,"elapsed":941,"user":{"displayName":"VR AI IEMA","userId":"08782298689493191086"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["import string\n","\n","# load doc into memory\n","def load_doc(filename):\n","\t# open the file as read only\n","\tfile = open(filename, 'r')\n","\t# read all text\n","\ttext = file.read()\n","\t# close the file\n","\tfile.close()\n","\treturn text\n","\n","# turn a doc into clean tokens\n","def clean_doc(doc):\n","\t# replace '--' with a space ' '\n","\tdoc = doc.replace('--', ' ')\n","\t# split into tokens by white space\n","\ttokens = doc.split()\n","\t# remove punctuation from each token\n","\ttable = str.maketrans('', '', string.punctuation)\n","\ttokens = [w.translate(table) for w in tokens]\n","\t# remove remaining tokens that are not alphabetic\n","\ttokens = [word for word in tokens if word.isalpha()]\n","\t# make lower case\n","\ttokens = [word.lower() for word in tokens]\n","\treturn tokens\n","\n","# save tokens to file, one dialog per line\n","def save_doc(lines, filename):\n","\tdata = '\\n'.join(lines)\n","\tfile = open(filename, 'w')\n","\tfile.write(data)\n","\tfile.close()\n","\n","# load document\n","in_filename ='/content/drive/MyDrive/Twitter-Sentiment-Analysis/Word-Level-Text-Generation/Plato.txt'\n","doc = load_doc(in_filename)\n","print(doc[:200])\n","\n","# clean document\n","tokens = clean_doc(doc)\n","print(tokens[:200])\n","print('Total Tokens: %d' % len(tokens))\n","print('Unique Tokens: %d' % len(set(tokens)))\n","\n","# organize into sequences of tokens\n","length = 50 + 1\n","sequences = list()\n","for i in range(length, len(tokens)):\n","\t# select sequence of tokens\n","\tseq = tokens[i-length:i]\n","\t# convert into a line\n","\tline = ' '.join(seq)\n","\t# store\n","\tsequences.append(line)\n","print('Total Sequences: %d' % len(sequences))\n","\n","# save sequences to file\n","out_filename = 'republic_sequences.txt'\n","save_doc(sequences, out_filename)"],"metadata":{"id":"HcYmOBEVMjgR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650357808778,"user_tz":-330,"elapsed":2461,"user":{"displayName":"VR AI IEMA","userId":"08782298689493191086"}},"outputId":"bb65176b-bed8-4018-b746-e028d871bf1d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The Project Gutenberg eBook of The Republic, by Plato\n","\n","This eBook is for the use of anyone anywhere in the United States and\n","most other parts of the world at no cost and with almost no restrictions\n","wh\n","['the', 'project', 'gutenberg', 'ebook', 'of', 'the', 'republic', 'by', 'plato', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'united', 'states', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 'reuse', 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'wwwgutenbergorg', 'if', 'you', 'are', 'not', 'located', 'in', 'the', 'united', 'states', 'you', 'will', 'have', 'to', 'check', 'the', 'laws', 'of', 'the', 'country', 'where', 'you', 'are', 'located', 'before', 'using', 'this', 'ebook', 'title', 'the', 'republic', 'author', 'plato', 'translator', 'b', 'jowett', 'release', 'date', 'october', 'ebook', 'most', 'recently', 'updated', 'september', 'language', 'english', 'produced', 'by', 'sue', 'asscher', 'and', 'david', 'widger', 'start', 'of', 'the', 'project', 'gutenberg', 'ebook', 'the', 'republic', 'the', 'republic', 'by', 'plato', 'translated', 'by', 'benjamin', 'jowett', 'note', 'see', 'also', 'by', 'plato', 'jowett', 'ebook', 'contents', 'introduction', 'and', 'analysis', 'the', 'republic', 'persons', 'of', 'the', 'dialogue', 'book', 'i', 'book', 'ii', 'book', 'iii', 'book', 'iv', 'book', 'v', 'book', 'vi', 'book', 'vii', 'book', 'viii', 'book', 'ix', 'book', 'x', 'introduction', 'and', 'analysis', 'the', 'republic', 'of', 'plato', 'is', 'the', 'longest', 'of', 'his', 'works', 'with', 'the', 'exception', 'of', 'the', 'laws', 'and', 'is', 'certainly', 'the', 'greatest', 'of', 'them', 'there', 'are', 'nearer', 'approaches', 'to']\n","Total Tokens: 216374\n","Unique Tokens: 10487\n","Total Sequences: 216323\n"]}]},{"cell_type":"code","source":["from numpy import array\n","from pickle import dump\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.utils import to_categorical\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.layers import Embedding\n","\n","# load doc into memory\n","def load_doc(filename):\n","\t# open the file as read only\n","\tfile = open(filename, 'r')\n","\t# read all text\n","\ttext = file.read()\n","\t# close the file\n","\tfile.close()\n","\treturn text\n","\n","# load\n","in_filename = 'republic_sequences.txt'\n","doc = load_doc(in_filename)\n","lines = doc.split('\\n')\n","\n","# integer encode sequences of words\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(lines)\n","sequences = tokenizer.texts_to_sequences(lines)\n","# vocabulary size\n","vocab_size = len(tokenizer.word_index) + 1\n","\n","# separate into input and output\n","sequences = array(sequences)\n","X, y = sequences[:,:-1], sequences[:,-1]\n","y = to_categorical(y, num_classes=vocab_size)\n","seq_length = X.shape[1]\n","\n","# define model\n","model = Sequential()\n","model.add(Embedding(vocab_size, 50, input_length=seq_length))\n","model.add(LSTM(100, return_sequences=True))\n","model.add(LSTM(100))\n","model.add(Dense(100, activation='relu'))\n","model.add(Dense(vocab_size, activation='softmax'))\n","print(model.summary())\n","# compile model\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","# fit model\n","model.fit(X, y, batch_size=128, epochs=100)\n","\n","# save the model to file\n","model.save('model.h5')\n","# save the tokenizer\n","dump(tokenizer, open('tokenizer.pkl', 'wb'))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tVukBkQO_bsp","executionInfo":{"status":"ok","timestamp":1650360484298,"user_tz":-330,"elapsed":2601608,"user":{"displayName":"VR AI IEMA","userId":"08782298689493191086"}},"outputId":"4bb08251-d10d-4afc-bf58-f71fa1b26cd7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 50, 50)            524400    \n","                                                                 \n"," lstm (LSTM)                 (None, 50, 100)           60400     \n","                                                                 \n"," lstm_1 (LSTM)               (None, 100)               80400     \n","                                                                 \n"," dense (Dense)               (None, 100)               10100     \n","                                                                 \n"," dense_1 (Dense)             (None, 10488)             1059288   \n","                                                                 \n","=================================================================\n","Total params: 1,734,588\n","Trainable params: 1,734,588\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Epoch 1/100\n","1691/1691 [==============================] - 32s 15ms/step - loss: 6.1256 - accuracy: 0.0990\n","Epoch 2/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 5.6628 - accuracy: 0.1330\n","Epoch 3/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 5.4534 - accuracy: 0.1505\n","Epoch 4/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 5.3172 - accuracy: 0.1602\n","Epoch 5/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 5.2029 - accuracy: 0.1680\n","Epoch 6/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 5.1155 - accuracy: 0.1743\n","Epoch 7/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 5.0240 - accuracy: 0.1782\n","Epoch 8/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 4.9486 - accuracy: 0.1806\n","Epoch 9/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 4.8647 - accuracy: 0.1845\n","Epoch 10/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 4.7894 - accuracy: 0.1876\n","Epoch 11/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 4.7129 - accuracy: 0.1902\n","Epoch 12/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 4.6564 - accuracy: 0.1921\n","Epoch 13/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 4.6019 - accuracy: 0.1948\n","Epoch 14/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 4.5394 - accuracy: 0.1968\n","Epoch 15/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 4.4861 - accuracy: 0.1989\n","Epoch 16/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 4.4371 - accuracy: 0.2005\n","Epoch 17/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 4.4086 - accuracy: 0.2025\n","Epoch 18/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 4.3590 - accuracy: 0.2045\n","Epoch 19/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 4.3358 - accuracy: 0.2072\n","Epoch 20/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 4.3020 - accuracy: 0.2098\n","Epoch 21/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 4.2654 - accuracy: 0.2120\n","Epoch 22/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 4.2361 - accuracy: 0.2142\n","Epoch 23/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 4.1844 - accuracy: 0.2169\n","Epoch 24/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 4.1566 - accuracy: 0.2194\n","Epoch 25/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 4.2541 - accuracy: 0.2140\n","Epoch 26/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 4.2491 - accuracy: 0.2132\n","Epoch 27/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 4.1765 - accuracy: 0.2176\n","Epoch 28/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 4.1181 - accuracy: 0.2220\n","Epoch 29/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 4.0799 - accuracy: 0.2246\n","Epoch 30/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 4.0395 - accuracy: 0.2281\n","Epoch 31/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.9982 - accuracy: 0.2313\n","Epoch 32/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.9605 - accuracy: 0.2351\n","Epoch 33/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.9335 - accuracy: 0.2375\n","Epoch 34/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.9077 - accuracy: 0.2404\n","Epoch 35/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.8903 - accuracy: 0.2431\n","Epoch 36/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.9014 - accuracy: 0.2421\n","Epoch 37/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.9514 - accuracy: 0.2381\n","Epoch 38/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 4.0767 - accuracy: 0.2267\n","Epoch 39/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 4.0137 - accuracy: 0.2308\n","Epoch 40/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.9184 - accuracy: 0.2392\n","Epoch 41/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.8981 - accuracy: 0.2418\n","Epoch 42/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.8584 - accuracy: 0.2449\n","Epoch 43/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.8045 - accuracy: 0.2503\n","Epoch 44/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.7645 - accuracy: 0.2547\n","Epoch 45/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.8195 - accuracy: 0.2496\n","Epoch 46/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.8308 - accuracy: 0.2483\n","Epoch 47/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.8453 - accuracy: 0.2467\n","Epoch 48/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.8629 - accuracy: 0.2449\n","Epoch 49/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.8312 - accuracy: 0.2478\n","Epoch 50/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.7980 - accuracy: 0.2508\n","Epoch 51/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.7633 - accuracy: 0.2540\n","Epoch 52/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.7293 - accuracy: 0.2578\n","Epoch 53/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.7000 - accuracy: 0.2611\n","Epoch 54/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.6744 - accuracy: 0.2646\n","Epoch 55/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.6476 - accuracy: 0.2669\n","Epoch 56/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.6193 - accuracy: 0.2705\n","Epoch 57/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.5862 - accuracy: 0.2752\n","Epoch 58/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.5597 - accuracy: 0.2781\n","Epoch 59/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.5343 - accuracy: 0.2807\n","Epoch 60/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.5068 - accuracy: 0.2845\n","Epoch 61/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.4815 - accuracy: 0.2882\n","Epoch 62/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.4559 - accuracy: 0.2913\n","Epoch 63/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.4319 - accuracy: 0.2938\n","Epoch 64/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.4070 - accuracy: 0.2976\n","Epoch 65/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.3824 - accuracy: 0.3008\n","Epoch 66/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.3621 - accuracy: 0.3035\n","Epoch 67/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.3378 - accuracy: 0.3066\n","Epoch 68/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.3127 - accuracy: 0.3101\n","Epoch 69/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.2935 - accuracy: 0.3126\n","Epoch 70/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.2698 - accuracy: 0.3153\n","Epoch 71/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.2565 - accuracy: 0.3174\n","Epoch 72/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.2237 - accuracy: 0.3225\n","Epoch 73/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.2016 - accuracy: 0.3252\n","Epoch 74/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.1826 - accuracy: 0.3285\n","Epoch 75/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.1606 - accuracy: 0.3316\n","Epoch 76/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.1431 - accuracy: 0.3339\n","Epoch 77/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.1213 - accuracy: 0.3373\n","Epoch 78/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.1011 - accuracy: 0.3403\n","Epoch 79/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.0826 - accuracy: 0.3438\n","Epoch 80/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.0635 - accuracy: 0.3456\n","Epoch 81/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.0453 - accuracy: 0.3477\n","Epoch 82/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.0232 - accuracy: 0.3515\n","Epoch 83/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 3.0055 - accuracy: 0.3537\n","Epoch 84/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 2.9899 - accuracy: 0.3565\n","Epoch 85/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 2.9728 - accuracy: 0.3589\n","Epoch 86/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 2.9530 - accuracy: 0.3630\n","Epoch 87/100\n","1691/1691 [==============================] - 25s 15ms/step - loss: 2.9387 - accuracy: 0.3642\n","Epoch 88/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 2.9186 - accuracy: 0.3677\n","Epoch 89/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 2.9014 - accuracy: 0.3710\n","Epoch 90/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 2.8844 - accuracy: 0.3732\n","Epoch 91/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 2.8699 - accuracy: 0.3747\n","Epoch 92/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 2.8547 - accuracy: 0.3777\n","Epoch 93/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 2.8394 - accuracy: 0.3812\n","Epoch 94/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 2.8238 - accuracy: 0.3827\n","Epoch 95/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 2.8041 - accuracy: 0.3863\n","Epoch 96/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 2.7919 - accuracy: 0.3879\n","Epoch 97/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 2.7770 - accuracy: 0.3906\n","Epoch 98/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 2.7612 - accuracy: 0.3939\n","Epoch 99/100\n","1691/1691 [==============================] - 26s 15ms/step - loss: 2.7489 - accuracy: 0.3950\n","Epoch 100/100\n","1691/1691 [==============================] - 25s 15ms/step - loss: 2.7365 - accuracy: 0.3969\n"]}]},{"cell_type":"code","source":["from random import randint\n","from pickle import load\n","from keras.models import load_model\n","from keras.preprocessing.sequence import pad_sequences\n","import numpy as np\n","\n","def AbstractiveLSTMTextGenerator(input_seed_text, max_length, text_file_cleaned, model_path, tokenizer_path):\n","\n","    # load doc into memory\n","    def load_doc(filename):\n","        # open the file as read only\n","        file = open(filename, 'r')\n","        # read all text\n","        text = file.read()\n","        # close the file\n","        file.close()\n","        return text\n","\n","    # generate a sequence from a language model\n","    def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n","        result = list()\n","        in_text = seed_text\n","        # generate a fixed number of words\n","        for _ in range(n_words):\n","            # encode the text as integer\n","            encoded = tokenizer.texts_to_sequences([in_text])[0]\n","            # truncate sequences to a fixed length\n","            encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n","            # predict probabilities for each word\n","            yhat = np.argmax(model.predict(encoded), axis=-1)\n","            # map predicted word index to word\n","            out_word = ''\n","            for word, index in tokenizer.word_index.items():\n","                if index == yhat:\n","                    out_word = word\n","                    break\n","            # append to input\n","            in_text += ' ' + out_word\n","            result.append(out_word)\n","        return ' '.join(result)\n","\n","\n","    # load cleaned text sequences\n","    in_filename = text_file_cleaned\n","    doc = load_doc(in_filename)\n","    lines = doc.split('\\n')\n","    seq_length = len(lines[0].split()) - 1\n","\n","    # load the model\n","    model = load_model(model_path)\n","\n","    # load the tokenizer\n","    tokenizer = load(open(tokenizer_path, 'rb'))\n","\n","    # # select a seed text\n","    # seed_text = lines[randint(0,len(lines))]\n","    # print(seed_text + '\\n')\n","\n","    seed_text = input_seed_text\n","\n","    # generate new text\n","    generated = generate_seq(model, tokenizer, seq_length, seed_text, max_length)\n","\n","    print(f\"The AI generated text is : {generated}\")\n","\n","    text_final = seed_text + \" \" + generated\n","    print(\"\\n\")\n","    print(\"The entire text is : {}\".format(text_final))\n","\n","    return text_final\n","\n"],"metadata":{"id":"nvFscAiI__Na"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","SEED = 'The man was constantly shooting with his gun'\n","\n","generated_text = AbstractiveLSTMTextGenerator(input_seed_text=SEED, \n","                                              max_length=175, \n","                                              text_file_cleaned='republic_sequences.txt', \n","                                              model_path='model.h5', tokenizer_path='tokenizer.pkl')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Od4GPe4Xb0Pq","executionInfo":{"status":"ok","timestamp":1650367671581,"user_tz":-330,"elapsed":8296,"user":{"displayName":"VR AI IEMA","userId":"08782298689493191086"}},"outputId":"01e29b5d-037e-4373-babe-953d518a74c0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The AI generated text is : own age and the companion of the world and the remainder of the human race in the clouds and the other absorbed for the practicability of the soul is constructed and sung by degrees the scattered institution of inconceivable technical diseases in the same part posted with the analysis of the pretence to the report of the earlier faculties in the republic the second and oblong cause milton in the republic the desirableness of mind is the rim circle pages by side arithmetical or denounced the truth in colour eg debt and andromache and the rest of the theory of human infant the philosopher is sufficiently evident the true helmsmen to them in conformity to falsehood the world of lectures and the most miserable of days is the entire christians is apt to blink at the affairs of the sexes and of xenophon and at first sight is only a logical difficulty in accordance with the analysis of the human race the illustrations of the republic is developed out of the uncertainty of all history\n","\n","\n","The entire text is : The man was constantly shooting with his gun own age and the companion of the world and the remainder of the human race in the clouds and the other absorbed for the practicability of the soul is constructed and sung by degrees the scattered institution of inconceivable technical diseases in the same part posted with the analysis of the pretence to the report of the earlier faculties in the republic the second and oblong cause milton in the republic the desirableness of mind is the rim circle pages by side arithmetical or denounced the truth in colour eg debt and andromache and the rest of the theory of human infant the philosopher is sufficiently evident the true helmsmen to them in conformity to falsehood the world of lectures and the most miserable of days is the entire christians is apt to blink at the affairs of the sexes and of xenophon and at first sight is only a logical difficulty in accordance with the analysis of the human race the illustrations of the republic is developed out of the uncertainty of all history\n"]}]},{"cell_type":"code","source":["generated_text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":140},"id":"SvpWoJZTVMZa","executionInfo":{"status":"ok","timestamp":1650367679721,"user_tz":-330,"elapsed":627,"user":{"displayName":"VR AI IEMA","userId":"08782298689493191086"}},"outputId":"dd40cb64-fd76-461b-8355-119d474e9e41"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'The man was constantly shooting with his gun own age and the companion of the world and the remainder of the human race in the clouds and the other absorbed for the practicability of the soul is constructed and sung by degrees the scattered institution of inconceivable technical diseases in the same part posted with the analysis of the pretence to the report of the earlier faculties in the republic the second and oblong cause milton in the republic the desirableness of mind is the rim circle pages by side arithmetical or denounced the truth in colour eg debt and andromache and the rest of the theory of human infant the philosopher is sufficiently evident the true helmsmen to them in conformity to falsehood the world of lectures and the most miserable of days is the entire christians is apt to blink at the affairs of the sexes and of xenophon and at first sight is only a logical difficulty in accordance with the analysis of the human race the illustrations of the republic is developed out of the uncertainty of all history'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":[""],"metadata":{"id":"SJp_5spuWJUC"},"execution_count":null,"outputs":[]}]}